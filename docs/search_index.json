[["index.html", "ウェブ最適化ではじめる機械学習 in R 概要", " ウェブ最適化ではじめる機械学習 in R Masayuki Sakai 2023-02-08 概要 本稿は(飯塚修平 2020)の学習と実装をRで行ったものをまとめたものである． また，このbookの内容はRの実装も含めてGitHubのリポジトリで公開している． References "],["id_01-intro.html", "1 A/Bテストから始めよう：ベイズ統計による仮説検定入門", " 1 A/Bテストから始めよう：ベイズ統計による仮説検定入門 A/Bテストはシンプルで実装も簡単 ビジネスに大きな影響をもたらしうる強力な手法 正しく実行することは難しい 本章ではA/Bテストを用いて数理的に意思決定する方法を学ぶ． "],["id_01-01-situation.html", "1.1 状況設定", " 1.1 状況設定 ここでは以下のような状況を想定して議論を進める． あるECサイトがあり，企業Xが運営している． Xの従業員2人がそれぞれ異なるページの改善を目的にA/Bテストを実施，レポートを作成した． Table 1.1: AliceとBobのレポート analyst description A B Alice クリック数 2.00 4.00 Alice クリック率 0.05 0.08 Alice 表示回数 40.00 50.00 Bob クリック数 64.00 128.00 Bob クリック率 0.05 0.08 Bob 表示回数 1280.00 1600.00 どちらの結果もB案の方がクリック率の意味で良い，という内容になっている． しかし，試行回数を見ると明らかにBobの方が多く信頼できそうである． 一方，Aliceの方の結果も正しいとするならば，より少ないサンプルで答えを導き出せた という意味で優秀と考えられる． 1.1.1 データ生成のプロセス あるページについて異なるデザイン（A/B）を作成してテストした結果，クリック率が異なる． という状況から，デザインが異なれば（真の）クリック率も異なるという仮説が立てられる． ただし，この段階ではテストの結果がそのまま妥当なのかどうかは不明である．\\(\\text{ctr}_A &gt; \\text{ctr}_B\\)とか\\(\\text{ctr}_A &lt; \\text{ctr}_B\\)のような関係の正当性は主張できない． いま\\(\\theta_i = \\text{ctr}_i\\)と表しておくと，これは割合であることから \\[ \\theta_i \\in [0, 1] \\] とできる．これはあるユーザーがページ\\(i\\)を訪れたときにクリックする確率と解釈できる． また実際にクリックした場合を\\(r=1\\)，しなかった場合を\\(r=0\\)としておく． ここでは単純にクリック\\(r\\)が生起確率\\(\\theta\\)のベルヌーイ分布\\(p(r|\\theta)\\)に従うと考えていく． また，\\(\\theta\\)自体も確率変数であり\\(p(\\theta)\\)というある確率分布に従うと考える． \\(p(\\theta)\\)をどのように設定するかは分析者に委ねられる． "],["id_01-02-pdf-approximation.html", "1.2 離散化による確率密度関数の近似", " 1.2 離散化による確率密度関数の近似 以下のような関数\\(f:[-1, 1] \\rightarrow [0, 1]\\)を考えてみる． \\[ f(x) = \\begin{cases} x + 1 &amp; (-1 \\leq x \\leq 0) \\\\ -x + 1 &amp; (0 &lt; x \\leq 1) \\end{cases} \\] この時定義域\\([-1, 1]\\)を適当な数\\(n\\)で均等に分割して，その中央値をその領域の関数値とした階段関数で近似すると以下のようになる． sample_n &lt;- 1000 f &lt;- function(x) { ans &lt;- ifelse(x &lt;= 0, x+1, -x+1) return(ans) } x &lt;- seq(-1, 1, length.out=sample_n) y &lt;- f(x) splits_n &lt;- 21 splits_x &lt;- seq(-1, 1, length.out=splits_n) splits_y &lt;- apply(cbind(splits_x[1:(splits_n-1)], splits_x[2:splits_n]), 1, function(x){f(median(x))}) discretize_x &lt;- splits_x[c(1,1, rep(2:(splits_n-1), each=4), splits_n, splits_n)] discretize_y &lt;- as.vector(rbind(0, splits_y, splits_y, 0)) label &lt;- c(rep(&quot;f(x)&quot;, length(x)), rep(&quot;Discretized f(x)&quot;, length(discretize_x))) data.frame( x=c(x, discretize_x), y=c(y, discretize_y), f=factor(label, levels=c(&quot;f(x)&quot;, &quot;Discretized f(x)&quot;)) ) %&gt;% ggplot(aes(x=x, y=y, group=f)) + geom_line(aes(color=f)) Figure 1.1: pdf \\(f(x)\\)と離散化近似した関数 splits_n &lt;- 21 x &lt;- seq(-1, 1, length.out=splits_n) x &lt;- apply(cbind(x[1:(splits_n-1)], x[2:splits_n]), 1, median) y &lt;- f(x) data.frame( x = x, y = y ) %&gt;% ggplot(aes(x=x, y=y)) + geom_bar(stat = &quot;identity&quot;) Figure 1.2: \\(f(x)\\)を確率質量関数で近似した結果 "],["id_01-03-additve-and-multiplication-theorem.html", "1.3 加法定理・乗法定理", " 1.3 加法定理・乗法定理 確率変数\\(X, Y\\)を考え，それぞれ\\(\\Omega_X = \\{X_1, \\ldots, X_n\\}, \\Omega_Y = \\{Y_1, \\ldots, Y_n \\}\\)の中から実現値を得るものとする． この時\\(X_i, Y_j\\)を同時に取る確率を\\(p(X=X_i, Y=Y_j)\\)と表し，これを同時確率と呼ぶ． またこの分布\\(p(X,Y)\\)を同時分布と呼ぶ． \\[ \\sum_{x \\in \\Omega_X, y \\in \\Omega_Y} p(X=x, Y=x) \\] とできる時，確率変数\\(Y\\)については \\[ \\begin{align} p(Y) = \\sum_{x \\in \\Omega_X} p(X, Y) \\tag{1.1} \\end{align} \\] と表すことができる．また条件付き分布（conditional distribution）を \\[ \\begin{align} p(X|Y) = \\frac{p(X,Y)}{p(Y)} \\tag{1.2} \\end{align} \\] と表し，確率の乗法定理とも呼ばれる． 1.3.1 周辺化・周辺分布 加法定理を適用して同時分布である確率変数のみの分布を表現することを周辺化（marginalization）と呼ぶ．このようにして得られた分布を周辺分布などと呼ぶ． 1.3.2 連続値の場合 連続型の確率変数\\(x, y\\)についても加法定理と乗法定理は同様に考えることができる． \\[ \\begin{align} p(y) &amp;= \\int_{\\infty}^{-\\infty} p(x,y) dx \\\\ p(x|y) &amp;= {p(x,y) \\over p(y)} \\tag{1.3} \\end{align} \\] 1.3.3 ベイズの定理 乗法定理は確率変数を交換しても成立する． \\[\\begin{align} \\tag{1.4} p(x,y) = p(x|y)p(y) = p(y|x)p(x) \\end{align}\\] これよりベイズの定理と呼ばれる次式を得る． \\[\\begin{align} \\tag{1.5} p(x|y) = \\dfrac{p(y|x)p(x)}{p(y)} \\end{align}\\] ベイズの定理を用いて，ある確率分布の未知パラメータを観測データから推論することをベイズ推論（Bayesian inference）と呼ぶ． ここで，観測データを\\(\\mathcal D\\)，未知パラメータを\\(\\theta\\)としてベイズの定理に当てはめると \\[\\begin{align} \\tag{1.6} p(\\theta | \\mathcal D) = \\dfrac{p(\\mathcal D | \\theta)}{p(\\mathcal D)}p(\\theta) \\end{align}\\] となる．つまり求めたいのは\\(p(\\theta|\\mathcal D)\\)である． \\(p(\\cdot)\\) description \\(p(\\theta)\\) 事前分布 \\(p(\\theta|\\mathcal D)\\) 事後分布 \\(p(\\mathcal D | \\theta)\\) 尤度 \\(p(\\mathcal D)\\) 正規化定数．これは\\(p(\\mathcal D| \\theta)p(\\theta)\\)を周辺化して得られる． 推論においてはデータ\\(\\mathcal D\\)は所与のものと考えるので，\\(p(\\mathcal D)\\)は定数とみなせること，また確率変数\\(\\theta\\)の分布のみに興味があることを強調して \\[\\begin{align} \\tag{1.7} p(\\theta|\\mathcal D) \\propto p(\\mathcal D | \\theta) p(\\theta) \\end{align}\\] と表現することもある． "],["id_01-04-bayse-inference.html", "1.4 ベイズの定理を使ったクリック率の推論", " 1.4 ベイズの定理を使ったクリック率の推論 推論を実行する際に，最初にどのような事前分布を設定するかを考える必要がある．これは分析者次第となる．よく使われるものの一つとして無情報の事前分布といういみで一様分布を設定することがあり，ここではこれを採用する． \\(\\theta\\)はクリック率を表す確率変数であり，\\(\\theta \\in [0,1]\\)なので\\(U(0,1)\\)を設定しよう． \\[\\begin{align} \\tag{1.8} p(\\theta) = \\begin{cases} 1 &amp; 0 \\leq \\theta \\leq 1 \\\\ 0 &amp; \\text{else} \\end{cases} \\end{align}\\] さらにこの\\(\\theta\\)はクリック率を意味するので \\[\\begin{align} \\tag{1.9} \\text{Bernoulli}(\\theta) = \\theta^r(1-\\theta)^{(1-r)} \\end{align}\\] 推論を行うとき，パラメータが与えられた時の\\(r\\)が発生する条件付き確率から，データが与えられた時に\\(\\theta\\)を推定することと読み替えることになる． つまり\\(p(\\mathcal D|\\theta)\\)ではなく\\(p(\\theta|\\mathcal D)\\)を考える．これを尤度関数と呼び，一般には確率分布ではなくなる． scene &lt;- list( &quot;xaxis&quot; = list(title=&quot;r&quot;), &quot;yaxis&quot; = list(title=&quot;theta&quot;), &quot;zaxis&quot; = list(title=&quot;p(r|theta)&quot;) ) n &lt;- 300 r &lt;- c(0,1) theta &lt;- seq(0,1,length=n) z &lt;- expand.grid(r,theta) %&gt;% apply(1, function(e){ res &lt;- e[2]^{e[1]}*(1-e[2])^{1-e[1]} return(res) }) %&gt;% matrix(nrow=n, byrow=T) plot_ly(x=r, y=theta, z=z, hovertemplate = &quot;r: %{x}&lt;br&gt;theta: %{y}&lt;br&gt;p(r|theta): %{z}&quot;) %&gt;% plotly::add_surface() %&gt;% plotly::layout(scene=scene) Figure 1.3: ベルヌーイ分布の3D表現 \\(p(r|\\theta)\\)を尤度関数とみなす，つまり\\(r\\)を固定した時の\\(\\theta\\)の関数と考えた時，\\(r=1\\)であれば \\[\\begin{align} \\int_{0}^{1} \\theta^{r} (1-\\theta)^{1-r} d\\theta &amp;= \\begin{cases} \\int_{0}^{1} \\theta d\\theta, &amp; r=1 \\\\ \\int_{0}^{1} (1-\\theta) d\\theta, &amp; r=0 \\end{cases} \\\\ &amp;= \\dfrac12 \\end{align}\\] となる．全積分が1ではないため，確率分布ではないことがわかる． さてこれらの尤度関数と事前分布を@ref{eq:bayse-inference-with-propto}式に代入すると，\\(\\theta \\in [0,1]\\)の範囲において考えれば\\(p(\\theta)=1\\)なので \\[\\begin{align} p(\\theta | r) &amp;= \\propto p(r | \\theta) p(\\theta) \\\\ &amp;= \\theta^r(1-\\theta)^{1-r} \\end{align}\\] となる． 先ほどの議論と同様にこの関数は確率分布ではないため，事後分布とするために正規化定数\\(p(r)\\)で除する必要がある． \\[\\begin{align} p(r) &amp;= \\int_{-\\infty}^{\\infty} p(r|\\theta) p(\\theta) d \\theta \\\\ &amp;= \\int_0^1 \\theta^r (1-\\theta)^{(1-r)} d \\theta \\\\ &amp;= \\begin{cases} \\displaystyle \\int_0^1 (1-\\theta) d \\theta, &amp; r=0 \\\\ \\displaystyle \\int_0^1 \\theta d \\theta, &amp; r=1 \\end{cases} \\\\ &amp;= \\dfrac12 \\end{align}\\] であるので，結局事後分布は \\[\\begin{align} p(\\theta | r) &amp;= \\dfrac{p(r|\\theta)p(\\theta)}{p(r)} \\\\ &amp;= 2 \\theta^r (1-\\theta)^{(1-r)} \\end{align}\\] と求められる．これらを図示すれば att_labs &lt;- c(&quot;r=1&quot;, &quot;r=0&quot;) names(att_labs) &lt;- c(1,0) r &lt;- c(0,1) theta &lt;- seq(0,1,length=300) df &lt;- expand_grid(r, theta) %&gt;% mutate(v = 2*theta^r * (1-theta)^{1-r}) g &lt;- ggplot(data=df, aes(x=theta, y=v,group=r)) + geom_line() + facet_wrap(~r, labeller = labeller(r=att_labs)) plot(g) Figure 1.4: ベイズ推論によって得られた事後分布 以上は一回のデータ観測におけるベイズ推論であったが実際にはいくつかのデータが得られているはずで，その分推論を繰り返すことでデータ\\(\\mathcal D\\)が観測された元での事後分布を得ることができる．ただしこの時，各データは独立であるという過程が置かれている． "],["id_01-05-implementation-in-r.html", "1.5 Rでの実装", " 1.5 Rでの実装 前項の例におけるベイズ推論を実装してみよう．まず，\\(\\theta\\)の取りうる値のベクトルを作成する．必ずしも1001分割でなければいけないわけではない． \\[ \\tilde{\\boldsymbol \\theta} = (0, 0001, 0.002,\\ldots,0.999, 1) \\] thetas &lt;- seq(0, 1, length=1001) thetas %&gt;% head() ## [1] 0.000 0.001 0.002 0.003 0.004 0.005 次に，尤度関数を実装する．実現値\\(r=1\\)であればthetasを，\\(r=0\\)なら1-thetasを返す． \\[ p(\\theta|r) = \\theta^r (1-\\theta)^{1-r} \\] likelihood &lt;- function(r, thetas){ if(r){ return(thetas) }else{ return(1-thetas) } } 最後に事後分布を計算するposteriorを実装する． \\[ p(\\theta|r) = 2\\theta^r(1-\\theta)^{1-r} \\] posterior &lt;- function(r, prior, thetas){ lp = likelihood(r, thetas) * prior return(lp / sum(lp)) # 尤度関数と事前分布の積をその和で割って正規化しておく } ベイズ推論を行う前に，事前分布を作成しておこう．事前分布は一様分布としていたので以下のようになる． prior &lt;- rep(1/length(thetas), length(thetas)) ではベイズ推論を実行する． p &lt;- posterior(r=1, prior, thetas) tibble(thetas=thetas, p=p, prior=prior) %&gt;% pivot_longer(cols=c(p,prior), names_to=&quot;distribution&quot;, values_to=&quot;prob&quot;) %&gt;% ggplot(aes(x=thetas, y=prob, group=distribution, color=distribution)) + geom_line() さらに繰り返しベイズ推論を実行して変化を見てみよう．いま得られたデータとしてはクリックが2，でクリックなしが38だったので，この順番には意味がないとすれば以下のように実装できる． len_n &lt;- 1001 thetas &lt;- seq(0,1,length=len_n) click = c(rep(1,2), rep(0, 38)) p &lt;- rep(1/len_n, len_n) results &lt;- tibble( thetas=thetas, p=p, iteration=0 ) for(i in 1:length(click)){ r &lt;- click[i] p &lt;- posterior(r=r, prior=p, thetas=thetas) results &lt;- bind_rows( results, tibble( thetas=thetas, p=p, iteration=i ) ) } 事後分布の変化の様子をプロットしてみよう． max_value &lt;- results %&gt;% dplyr::filter(iteration==max(iteration), p==max(p)) %&gt;% pull(thetas) results %&gt;% ggplot(aes(x=thetas, y=p, group=iteration, color=iteration)) + geom_line() + geom_vline(xintercept=max_value, color=&quot;orange&quot;) Figure 1.5: ベイズ推論の繰り返しによる事後分布の変遷 最終的には0.05付近に最も大きな値をもつ事後分布が得られていることがわかる（黄色の縦線）． "],["id_01-06-bayse-inference-with-reports.html", "1.6 レポートデータを用いた推論", " 1.6 レポートデータを用いた推論 表 1.1 のデータに基づいたベイズ推論を実行してみよう． 簡単のため，さらにメタ的な関数を実装しておく． また繰り返しの回数が多くなるので，２項分布を用いた形に書き直しておこう． likelihood &lt;- function(x, n, thetas){ res &lt;- (thetas^{x}) * ((1-thetas)^{n-x}) return(res) } posterior &lt;- function(x, n, prior, thetas){ lp = likelihood(x, n, thetas) * prior return(lp / sum(lp)) # 尤度関数と事前分布の積をその和で割って正規化しておく } click_rate_inference &lt;- function(click, imp, prior_type=&quot;uniform&quot;, len_n=1001){ thetas &lt;- seq(0,1,length=len_n) if(prior_type==&quot;uniform&quot;){ p &lt;- rep(1/len_n, len_n) } p &lt;- posterior(x=click, n=imp, prior=p, thetas=thetas) results &lt;- tibble( thetas=thetas, p=p ) return(results) } では，AliceとBobの二つのデザインについての結果を用いて推論を実行してみよう． reports &lt;- list( &quot;Alice-A&quot; = list(name=&quot;Alice&quot;, type=&quot;A&quot;, imp=40, click=2), &quot;Alice-B&quot; = list(name=&quot;Alice&quot;, type=&quot;B&quot;, imp=50, click=4), &quot;Bob-A&quot; = list(name=&quot;Bob&quot;, type=&quot;A&quot;, imp=1280, click=64), &quot;Bob-B&quot; = list(name=&quot;Bob&quot;, type=&quot;B&quot;, imp=1600, click=128) ) results &lt;- lapply(reports, function(e){ res &lt;- click_rate_inference(imp=e$imp, click=e$click) res$name &lt;- e$name res$type &lt;- e$type return(res) }) %&gt;% bind_rows() AliceとBobそれぞれについて事後分布を可視化してみよう． results %&gt;% dplyr::filter(thetas &lt; 0.3) %&gt;% # 0.3以上はほとんど0なので除く ggplot(aes(x=thetas, y=p, group=type, color=type)) + geom_line() + facet_wrap(~name, scales = &quot;free_y&quot;, nrow = 2) Figure 1.6: AliceとBobのレポートのベイズ推論 単純な集計では，AliceとBobのレポートではクリック率がA,Bで同じだったが，今回の推論による事後分布を見ると，より多くのデータを獲得できたBobの結果の方がより確信度が高いことがわかる．Aliceの方は分布の裾が大きいため，クリック率についての確信度はBobよりも高くない． "],["id_01-07-decision-making.html", "1.7 事後分布から決断を下す", " 1.7 事後分布から決断を下す ベイズ推論で得られた事後分布の特徴を数値化したり，事後分布からのサンプルを利用することで意思決定に有用な情報を得ることができる． 1.7.1 要約統計量 Definition 1.1 (期待値) 連続・離散の確率分布に対して期待値をそれぞれ以下のように定義する． \\[\\begin{align} E[x] = \\begin{cases} \\int_{-\\infty}^{\\infty} xp(x) dx, &amp; \\text{continuous} \\sum{x} x p(x), &amp; \\text{discrete} \\end{cases} \\end{align}\\] Definition 1.2 (分散) 分散は次のように定義される． \\[\\begin{align} V[x] = E[(x-E[x])^2] \\end{align}\\] また\\(\\sqrt{V[x]}\\)を標準偏差と呼ぶ． Definition 1.3 (標本平均・標本分散) 確率分布から得られたサンプルに対して，その平均\\(\\bar x\\)と分散\\(s^2\\)を次のように定義する．これらはそれぞれ標本平均・標本分散と呼ぶ． \\[\\begin{align} \\bar x &amp;= \\dfrac{1}{n}\\sum_{i=1}^{n} x_i \\\\ s^2 &amp;= \\dfrac1n \\sum_{i=1}^{n} (x_i - \\bar x)^2 \\end{align}\\] 標本平均については，サンプルが独立で同一の分布に従う(independent identity distribution)とき，\\(n \\rightarrow \\infty\\)で分布の期待値に収束することが知られている（大数の法則）． これを \\[\\begin{align} \\bar x \\longrightarrow E[x] \\ (n \\rightarrow \\infty) \\end{align}\\] と表す． Definition 1.4 (HDI:highest density interval) 連続型の確率分布に対して，以下のように定義される量をHDI(highest density interval)と呼ぶ． \\[\\begin{align} \\int_{p(x)&gt;t} p(x) dx = \\dfrac{\\alpha}{100} \\end{align}\\] ここで\\(t\\)は閾値でこの値よりも確率密度が高い区間で埋めていって\\(\\alpha\\)に一致する点となる． HDIは確率変数の値が高い確率で現れる区間を表していて，例えば\\(\\alpha = 0.95\\)のとき95%HDIと呼ばれ，確率\\(0.95\\)を占めるまでの確率密度が高いものから埋めて行った区域となる． 確率分布が複数の山（ピーク）を持つ場合，HDIは複数の区域に分かれることがある． 1.7.2 HDIをつかった仮設検定 本来連続型の確率分布に対して定義されているHDIだが，同様の考え方を用いで離散型の確率分布にも適用してみよう． ここでは，確率質量が多い確率変数の値を順に追加していく，という操作をする． hmv &lt;- function(xs, ps, alpha=0.95){ df &lt;- tibble(xs=xs, ps=ps) %&gt;% dplyr::arrange(desc(ps)) %&gt;% mutate(cumsum_ps = cumsum(ps)) hmv_range &lt;- df %&gt;% dplyr::filter(cumsum_ps &lt;= alpha) %&gt;% pull(xs) %&gt;% range() res &lt;- list( range=hmv_range, x=df ) return(res) } # AliceのA案の結果についてhmvを計算する Alice_A &lt;- results %&gt;% dplyr::filter( name == &quot;Alice&quot;, type == &quot;A&quot; ) hmv_Alice_a &lt;- hmv(xs=Alice_A$thetas, ps=Alice_A$p) hmv_Alice_a$range ## [1] 0.009 0.148 plots &lt;- list() i &lt;- 1 for(tgt_name in c(&quot;Alice&quot;, &quot;Bob&quot;)){ for(tgt_type in c(&quot;A&quot;, &quot;B&quot;)){ df &lt;- results %&gt;% dplyr::filter( name == tgt_name, type == tgt_type ) fill_range &lt;- hmv(xs=df$thetas, ps=df$p)$range bounds &lt;- df %&gt;% dplyr::filter( thetas &gt; fill_range[1], thetas &lt; fill_range[2] ) plots[[i]] &lt;- df %&gt;% dplyr::filter(thetas &lt; 0.3) %&gt;% ggplot(aes(x=thetas, y=p)) + geom_line() + geom_ribbon(data=bounds, aes(ymax=p), ymin=0, fill=&quot;orange&quot;, alpha=0.5) i &lt;- i+1 } } gridExtra::grid.arrange( plots[[1]],plots[[2]],plots[[3]],plots[[4]], nrow=4 ) "],["id_99_references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
